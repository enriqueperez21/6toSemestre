{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62825f45",
   "metadata": {},
   "source": [
    "**Nombre:** Luis Enrique Pérez Señalin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296bf380",
   "metadata": {},
   "source": [
    "# Chapter 6 Algorithm Test Harnesses\n",
    "\n",
    "We cannot know which algorithm will be best for a given problem. Therefore, we need to design a test harness that we can use to evaluate different machine learning algorithms. In this tutorial, you will discover how to develop a machine learning algorithm test harness from scratch in Perl. After completing this tutorial, you will know:\n",
    "\n",
    "- How to implement a train-test algorithm test harness.\n",
    "- How to implement a k-fold cross-validation algorithm test harness. Let’s get started.\n",
    "\n",
    "## 6.1 Description\n",
    "\n",
    "A test harness provides a consistent way to evaluate machine learning algorithms on a dataset. It involves 3 elements:\n",
    "\n",
    "1. The resampling method to split-up the dataset.\n",
    "2. The machine learning algorithm to evaluate.\n",
    "3. The performance measure by which to evaluate predictions.\n",
    "\n",
    "The loading and preparation of a dataset is a prerequisite step that must have been completed prior to using the test harness. The test harness must allow for different machine learning algorithms to be evaluated, whilst the dataset, resampling method and performance measures are kept constant. In this tutorial, we are going to demonstrate the test harnesses with a real dataset.\n",
    "\n",
    "### 6.1.1 Pima Indians Diabetes Dataset\n",
    "\n",
    "In this tutorial we will use the Pima Indians Diabetes Dataset. This dataset involves the prediction of the onset of diabetes within 5 years. The baseline performance on the problem is approximately 65%. You can learn more about it in Appendix A, Section A.4. Download the dataset and save it into your current working directory with the filename `pima-indians-diabetes.csv`.\n",
    "\n",
    "## 6.2 Tutorial\n",
    "\n",
    "This tutorial is broken down into two main sections:\n",
    "\n",
    "1. Train-Test Algorithm Test Harness.\n",
    "2. Cross-Validation Algorithm Test Harness. These test harnesses will give you the foundation that you need to evaluate a suite of machine learning algorithms on a given predictive modeling problem.\n",
    "\n",
    "### 6.2.1 Train-Test Algorithm Test Harness\n",
    "\n",
    "The train-test split is a simple resampling method that can be used to evaluate a machine learning algorithm. As such, it is a good starting point for developing a test harness. We can assume the prior development of a function to split a dataset into train and test sets and a function to evaluate the accuracy of a set of predictions. We need a function that can take a dataset and an algorithm and return a performance score. Below is a function named `evaluate_algorithm()` that achieves this. It takes 3 fixed arguments including the dataset, the algorithm function and the split percentage for the train-test split. \n",
    "\n",
    "First, the dataset is split into train and test elements. Next, a copy of the test set is made and each output value is cleared by setting it to the `None` value to prevent the algorithm from cheating accidentally. The algorithm provided as a parameter is a function that expects the train and test datasets on which to prepare and then make predictions. The algorithm may require additional configuration parameters. This is handled by using the variable arguments `%args` in the `evaluate_algorithm()` function and passing them on to the algorithm function. The algorithm function is expected to return a list of predictions, one for each row in the training dataset. These are compared to the actual output values from the unmodified test dataset by the `accuracy_metric()` function. Finally, the accuracy is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c4a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from sml import SML\n",
    "sml = SML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54700089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined in Section 6.2.1 Train-Test Algorithm Test Harness\n",
    "# Function To Evaluate An Algorithm Using a Train/Test Split.\n",
    "# Evaluate an algorithm using a train/test split\n",
    "def evaluate_algorithm_train_test_split( dataset, algorithm, *algo_posargs, split=0.6, metric=None, return_all=False, **algo_kwargs):\n",
    "    train, test = sml.train_test_split(dataset, split=split)\n",
    "\n",
    "    test_set = test.copy()\n",
    "    test_set[:, -1] = float('nan')\n",
    "\n",
    "    predicted = algorithm(train, test_set, *algo_posargs, **algo_kwargs)\n",
    "    # Y real del test\n",
    "    actual = test[:, -1]\n",
    "\n",
    "    # 5. Determinar métrica\n",
    "    if metric:\n",
    "        metric = metric.lower()\n",
    "        if 'accuracy' in metric:\n",
    "            score = SML.accuracy_metric(actual, predicted)\n",
    "        elif 'rmse' in metric:\n",
    "            score = SML.rmse_metric(actual, predicted)\n",
    "        else:\n",
    "            raise ValueError(\"Métrica no reconocida\")\n",
    "    else:\n",
    "        # Detectar si hay decimales\n",
    "        has_decimals = mx.nd.sum(actual != actual.astype('int64')).asscalar() > 0\n",
    "        score = (\n",
    "            SML.rmse_metric(actual, predicted)\n",
    "            if has_decimals\n",
    "            else SML.accuracy_metric(actual, predicted)\n",
    "        )\n",
    "    return (score, train, test, actual, predicted) if return_all else score\n",
    "\n",
    "sml.add_to_class(sml, 'evaluate_algorithm_train_test_split', evaluate_algorithm_train_test_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f19f96",
   "metadata": {},
   "source": [
    "The evaluation function does make some strong assumptions, but they can easily be changed if needed. Specifically, it assumes that the last row in the dataset is always the output value. A different column could be used. The use of the `accuracy_metric()` assumes that the problem is a classification problem, but this could be changed to mean squared error for regression problems.\n",
    "\n",
    "Let’s piece this together with a worked example. We will use the Pima Indians Diabetes dataset and evaluate the Zero Rule algorithm.\n",
    "\n",
    "The dataset was split into 60% for training the model and 40% for evaluating it. Notice how the name of the Zero Rule algorithm `zero_rule_algorithm_classification` was passed as an argument to the `evaluate_algorithm()` function. You can see how this test harness may be used again and again with different algorithms. Running the example above prints out the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f034e8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.96%\n"
     ]
    }
   ],
   "source": [
    "mx.random.seed(1)\n",
    "\n",
    "filename = \"./data/pima-indians-diabetes.csv\"   # ajusta la ruta si es necesario\n",
    "dataset, header = sml.load_csv(filename)\n",
    "\n",
    "for i in range(len(dataset[0]) - 1):\n",
    "    sml.str_column_to_float(dataset, i)\n",
    "\n",
    "dataset_nd = mx.nd.array(dataset, dtype='float32')\n",
    "\n",
    "split = 0.6\n",
    "accuracy, train, test, actual, predicted = sml.evaluate_algorithm_train_test_split(\n",
    "    dataset_nd,\n",
    "    sml.zero_rule_algorithm_classification,\n",
    "    split=split,\n",
    "    metric=\"accuracy\",\n",
    "    return_all=True\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19f861f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A/P 0 1\n",
      "0   197 0\n",
      "1   111 0\n"
     ]
    }
   ],
   "source": [
    "unique, matrix = sml.confusion_matrix(actual, predicted)\n",
    "sml.print_confusion_matrix(unique, matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef2fa8e",
   "metadata": {},
   "source": [
    "### 6.2.2 Cross-Validation Algorithm Test Harness\n",
    "\n",
    "Cross-validation is a resampling technique that provides more reliable estimates of algorithm performance on unseen data. It requires the creation and evaluation of k models on different subsets of your data, and as such is more computationally expensive. Nevertheless, it is the gold standard for evaluating machine learning algorithms.\n",
    "\n",
    "As in the previous section, we need to create a function that ties together the resampling method, the evaluation of the algorithm on the dataset and the performance calculation method. Unlike above, the algorithm must be evaluated on different subsets of the dataset many times. This means we need additional loops within our `evaluate_algorithm()` function.\n",
    "\n",
    "Below is a function that implements algorithm evaluation with cross-validation. First, the dataset is split into n folds groups called folds. Next, we loop giving each fold an opportunity to be held out of training and used to evaluate the algorithm. A copy of the list of folds is created and the held out fold is removed from this list. Then the list of folds is flattened into one long list of rows to match the algorithm’s expectation of a training dataset. This is done by using a dereferentiation of an array inside the `map()` function.\n",
    "\n",
    "Once the training dataset is prepared the rest of the function within this loop is as above. A copy of the test dataset (the fold) is made and the output values are cleared to avoid accidental cheating by algorithms. The algorithm is prepared on the train dataset and makes predictions on the test dataset. The predictions are evaluated and stored in a list. Unlike the train-test algorithm test harness, a list of scores is returned, one for each cross-validation fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b26dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_algorithm_cross_validation_split( dataset, algorithm, *algo_posargs, n_folds=10, metric=None, return_all=False, **algo_kwargs ):\n",
    "    folds = sml.cross_validation_split(dataset, n_folds=n_folds)\n",
    "    scores       = []\n",
    "    train_losses = []\n",
    "    test_losses  = []\n",
    "    predictions  = []\n",
    "    actuals      = []\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        train_set = mx.nd.concat(*[f for j, f in enumerate(folds) if j != i], dim=0)\n",
    "\n",
    "        test_set = fold.copy()\n",
    "        test_set[:, -1] = float(\"nan\")\n",
    "\n",
    "        returned = algorithm(train_set, test_set, *algo_posargs, **algo_kwargs)\n",
    "\n",
    "        if isinstance(returned, tuple):\n",
    "            predicted, train_loss, test_loss = (returned + (None, None))[:3]\n",
    "        else:\n",
    "            predicted, train_loss, test_loss = returned, None, None\n",
    "\n",
    "        # 4) etiquetas reales del fold\n",
    "        actual = fold[:, -1]\n",
    "\n",
    "        if metric:\n",
    "            m = metric.lower()\n",
    "            if \"accuracy\" in m:\n",
    "                score = sml.accuracy_metric(actual, predicted)\n",
    "            elif \"rmse\" in m:\n",
    "                score = sml.rmse_metric(actual, predicted)\n",
    "            else:\n",
    "                raise ValueError(\"Métrica no reconocida\")\n",
    "        else:\n",
    "            has_decimals = mx.nd.sum(actual != actual.astype(\"int64\")).asscalar() > 0\n",
    "            score = (\n",
    "                sml.rmse_metric(actual, predicted)\n",
    "                if has_decimals\n",
    "                else sml.accuracy_metric(actual, predicted)\n",
    "            )\n",
    "\n",
    "        # almacenar resultados de este fold\n",
    "        scores.append(score)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        predictions.append(predicted)\n",
    "        actuals.append(actual)\n",
    "    \n",
    "    if return_all:\n",
    "        return scores, train_losses, test_losses, actuals, predictions\n",
    "    return scores\n",
    "\n",
    "sml.add_to_class(sml, 'evaluate_algorithm_cross_validation_split', evaluate_algorithm_cross_validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60823d7",
   "metadata": {},
   "source": [
    "Although slightly more complex in code and slower to run, this function provides a more robust estimate of algorithm performance. We can tie all of this together with a complete example on the diabetes dataset with the Zero Rule algorithm.\n",
    "\n",
    "A total of 5 cross-validation folds were used to evaluate the Zero Rule Algorithm. As such, 5 scores were returned from the `evaluate_algorithm()` algorithm. Running this example both prints these list of scores calculated and prints the mean score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70a44133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: ['66.01', '62.75', '68.63', '62.75', '65.36']\n",
      "Mean Accuracy: 65.10%\n"
     ]
    }
   ],
   "source": [
    "mx.random.seed(1)\n",
    "\n",
    "filename = \"./data/pima-indians-diabetes.csv\"\n",
    "dataset, header = sml.load_csv(filename)\n",
    "\n",
    "for i in range(len(dataset[0]) - 1):\n",
    "    sml.str_column_to_float(dataset, i, precision=1)\n",
    "\n",
    "dataset_nd = mx.nd.array(dataset, dtype=\"float32\")\n",
    "\n",
    "n_folds = 5\n",
    "scores, train_losses, test_losses, actuals, predictions = \\\n",
    "    sml.evaluate_algorithm_cross_validation_split( dataset_nd, sml.zero_rule_algorithm_classification, n_folds=n_folds, metric=\"accuracy\", return_all=True )\n",
    "\n",
    "print(\"Scores:\", scores)\n",
    "mean_acc = sum(map(float, scores)) / len(scores)\n",
    "print(f\"Mean Accuracy: {mean_acc:0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d3f5561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.01%\n",
      "A/P 0 1\n",
      "0   101 0\n",
      "1   52 0\n",
      "Accuracy: 62.75%\n",
      "A/P 0 1\n",
      "0   96 0\n",
      "1   57 0\n",
      "Accuracy: 68.63%\n",
      "A/P 0 1\n",
      "0   105 0\n",
      "1   48 0\n",
      "Accuracy: 62.75%\n",
      "A/P 0 1\n",
      "0   96 0\n",
      "1   57 0\n",
      "Accuracy: 65.36%\n",
      "A/P 0 1\n",
      "0   100 0\n",
      "1   53 0\n"
     ]
    }
   ],
   "source": [
    "for accuracy, actual, predicted in zip(scores, actuals, predictions):\n",
    "    unique, matrix = sml.confusion_matrix(actual, predicted)\n",
    "    print(f\"Accuracy: {accuracy}%\")\n",
    "    sml.print_confusion_matrix(unique, matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f3861b",
   "metadata": {},
   "source": [
    "## 6.3 Extensions\n",
    "\n",
    "This section lists extensions to this tutorial that you may wish to consider.\n",
    "\n",
    "- **Parameterized Evaluation**. Pass in the function used to evaluate predictions, allowing you to seamlessly work with regression problems.\n",
    "- **Parameterized Resampling**. Pass in the function used to calculate resampling splits, allowing you to easily switch between the train-test and cross-validation methods.\n",
    "- **Standard Deviation Scores**. Calculate the standard deviation to get an idea of the spread of scores when evaluating algorithms using cross-validation.\n",
    "\n",
    "## 6.4 Review\n",
    "\n",
    "In this tutorial, you discovered how to create a test harness from scratch to evaluate your machine learning algorithms. Specifically, you now know:\n",
    "\n",
    "- How to implement and use a train-test algorithm test harness.\n",
    "- How to implement and use a cross-validation algorithm test harness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mxnet-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
