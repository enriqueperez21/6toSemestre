{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93faf43",
   "metadata": {},
   "source": [
    "# Chapter 2: Scale Machine Learning Data\n",
    "\n",
    "Many machine learning algorithms expect data to be scaled consistently. There are two popular\n",
    "methods that you should consider when scaling your data for machine learning. In this tutorial,\n",
    "you will discover how you can rescale your data for machine learning. After reading this tutorial\n",
    "you will know:\n",
    "\n",
    "* How to normalize your data from scratch.\n",
    "* How to standardize your data from scratch.\n",
    "* When to normalize as opposed to standardize data.\n",
    "\n",
    "Let’s get started.\n",
    "\n",
    "## 2.1 Description\n",
    "\n",
    "Many machine learning algorithms expect the scale of the input and even the output data to be\n",
    "equivalent. It can help in methods that weight inputs in order to make a prediction, such as\n",
    "in linear regression and logistic regression. It is practically required in methods that combine\n",
    "weighted inputs in complex ways such as in artificial neural networks and deep learning.\n",
    "\n",
    "### 2.1.1 Pima Indians Diabetes Dataset\n",
    "In this tutorial we will use the Pima Indians Diabetes Dataset. This dataset involves the predic-\n",
    "tion of the onset of diabetes within 5 years. The baseline performance on the problem is approx-\n",
    "imately 65%. You can learn more about it in Appendix A, Section A.4. Download the dataset\n",
    "and save it into your current working directory with the filename pima-indians-diabetes.csv.\n",
    "\n",
    "## 2.2 Tutorial\n",
    "\n",
    "This tutorial is divided into 3 parts:\n",
    "1. Normalize Data.\n",
    "2. Standardize Data.\n",
    "3. When to Normalize and Standardize.\n",
    "\n",
    "These steps will provide the foundations you need to handle scaling your own data.\n",
    "\n",
    "### 2.2.1 Normalize Data\n",
    "\n",
    "Normalization can refer to different techniques depending on context. Here, we use normalization\n",
    "to refer to rescaling an input variable to the range between 0 and 1. Normalization requires\n",
    "that you know the minimum and maximum values for each attribute.\n",
    "This can be estimated from training data or specified directly if you have deep knowledge\n",
    "of the problem domain. You can easily estimate the minimum and maximum values for each\n",
    "attribute in a dataset by enumerating through the values. The snippet of code below defines\n",
    "the dataset minmax() function that calculates the min and max value for each attribute in a\n",
    "dataset, then returns an array of these minimum and maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b69e3dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "use strict;\n",
    "use warnings;\n",
    "use PDL;\n",
    "use PDL::NiceSlice;\n",
    "use Data::Dump qw(dump);\n",
    "use sml; # Statistical Machine Learning Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d6f9a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "*sml::dataset_minmax"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "Warning",
     "evalue": "Subroutine sml::dataset_minmax redefined at /usr/local/lib/perl5/site_perl/5.32.1/x86_64-linux/sml.pm line 22.\n",
     "output_type": "error",
     "traceback": [
      "Subroutine sml::dataset_minmax redefined at /usr/local/lib/perl5/site_perl/5.32.1/x86_64-linux/sml.pm line 22.\n"
     ]
    }
   ],
   "source": [
    "# Función para calcular el mínimo y máximo de cada columna usando PDL\n",
    "sub dataset_minmax {\n",
    "    my ($dataset) = @_;\n",
    "    \n",
    "    # Transponer el tensor para que cada columna sea una fila temporal\n",
    "    my $transposed = $dataset->xchg(0, 1);\n",
    "\n",
    "    # Calcular mínimos y máximos directamente para cada columna\n",
    "    my $mins = $transposed->minimum;\n",
    "    my $maxs = $transposed->maximum;\n",
    "\n",
    "    # Combinar mínimos y máximos en un solo tensor\n",
    "    my $minmax = cat($mins, $maxs)->xchg(0, 1);\n",
    "\n",
    "    return $minmax;\n",
    "}\n",
    "sml->add_to_class('dataset_minmax', \\&{'dataset_minmax'});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30fb762",
   "metadata": {},
   "source": [
    "With this contrived dataset, we can test our function for calculating the min and max for\n",
    "each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4409fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      "\n",
      "[\n",
      " [50 30]\n",
      " [20 90]\n",
      "]\n",
      "Minimos y maximos por columna:\n",
      "\n",
      "[\n",
      " [20 50]\n",
      " [30 90]\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear un tensor de 2x2 usando PDL\n",
    "my $dataset = pdl [[50, 30], [20, 90]];\n",
    "print \"Dataset:\\n\", $dataset;\n",
    "\n",
    "# Calcular mínimos y máximos usando PDL directamente\n",
    "my $minmax = dataset_minmax($dataset);  # Llamada directa\n",
    "print \"Minimos y maximos por columna:\\n\", $minmax;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced51ed7",
   "metadata": {},
   "source": [
    "Once we have estimates of the maximum and minimum allowed values for each column, we\n",
    "can now normalize the raw data to the range 0 and 1. The calculation to normalize a single\n",
    "value for a column is:\n",
    "\n",
    "<center>$scaled\\ value = (value − min)\\ /\\ (max − min)$</center>  (2.1)\n",
    "\n",
    "Below is an implementation of this in a function called normalize dataset() that normalizes\n",
    "values in each column of a provided dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a63e823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para normalizar un conjunto de datos usando PDL\n",
    "sub normalize_dataset {\n",
    "    my ($dataset, $minmax) = @_;\n",
    "\n",
    "    # Extraer mínimos y máximos del tensor minmax\n",
    "    my $mins = $minmax->slice(':,0');  \n",
    "    my $maxs = $minmax->slice(':,1');  \n",
    "\n",
    "    # Expandir y repetir las dimensiones para que coincidan con el dataset\n",
    "    $mins = $mins->dummy(0);\n",
    "    $maxs = $maxs->dummy(0);\n",
    "    $mins = $mins->reshape($dataset->dim(0), $dataset->dim(1));\n",
    "    $maxs = $maxs->reshape($dataset->dim(0), $dataset->dim(1));\n",
    "\n",
    "    # Calcular el rango para cada columna\n",
    "    my $range = $maxs - $mins;\n",
    "\n",
    "    # Normalización: (valor - min) / (max - min)\n",
    "    my $normalized = ($dataset - $mins) / $range;\n",
    "\n",
    "    return $normalized;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774b4332",
   "metadata": {},
   "source": [
    "We can tie this function together with the dataset minmax() function and normalize the\n",
    "contrived dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b2854f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      "\n",
      "[\n",
      " [50 30]\n",
      " [20 90]\n",
      "]\n",
      "Minimos y maximos por columna:\n",
      "\n",
      "[\n",
      " [20 50]\n",
      " [30 90]\n",
      "]\n",
      "Dataset Normalizado:\n",
      "\n",
      "[\n",
      " [   3 -0.5]\n",
      " [ Inf  Inf]\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear un tensor de 2x2 usando PDL\n",
    "my $dataset = pdl [[50, 30], [20, 90]];\n",
    "print \"Dataset:\\n\", $dataset;\n",
    "\n",
    "# Calcular el mínimo y el máximo para cada columna\n",
    "my $minmax = dataset_minmax($dataset);\n",
    "print \"Minimos y maximos por columna:\\n\", $minmax;\n",
    "\n",
    "# Normalizar el conjunto de datos\n",
    "my $normalized = normalize_dataset($dataset, $minmax);\n",
    "print \"Dataset Normalizado:\\n\", $normalized;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6092d625",
   "metadata": {},
   "source": [
    "We can combine this code with code for loading a CSV dataset and load and normalize the\n",
    "Pima Indians Diabetes dataset. The example first loads the dataset and converts the values for\n",
    "each column from string to floating point values. The minimum and maximum values for each\n",
    "column are estimated from the dataset, and finally, the values in the dataset are normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19bfe51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file ./data/pima-indians-diabetes.csv with 9 rows and 768 columns.\n",
      "\n",
      "Dataset[0]:\n",
      "\n",
      "[\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 2]\n",
      " [ 8]\n",
      " [ 4]\n",
      " [10]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 7]\n",
      " [ 9]\n",
      " [11]\n",
      " [10]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [13]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [10]\n",
      " [ 4]\n",
      " [11]\n",
      " [ 9]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 9]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 5]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [13]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 5]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [13]\n",
      " [ 2]\n",
      " [15]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 9]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 7]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [17]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 6]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 8]\n",
      " [ 6]\n",
      " [ 0]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 9]\n",
      " [ 7]\n",
      " [11]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 9]\n",
      " [12]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 9]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 9]\n",
      " [10]\n",
      " [ 0]\n",
      " [ 9]\n",
      " [ 1]\n",
      " [ 9]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [12]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [11]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [13]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [10]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [14]\n",
      " [ 8]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 7]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [13]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [ 8]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [12]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 5]\n",
      " [ 9]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 9]\n",
      " [ 1]\n",
      " [13]\n",
      " [12]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [12]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 9]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 8]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [12]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 8]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [14]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 8]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 4]\n",
      " [ 8]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 8]\n",
      " [12]\n",
      " [ 0]\n",
      " [ 9]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 9]\n",
      " [ 7]\n",
      " [13]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 9]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [11]\n",
      " [11]\n",
      " [ 6]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 6]\n",
      " [12]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 6]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [11]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 7]\n",
      " [ 6]\n",
      " [11]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 9]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [10]\n",
      " [13]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [11]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [11]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [10]\n",
      " [ 6]\n",
      " [ 9]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 6]\n",
      " [ 9]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [13]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [10]\n",
      " [ 2]\n",
      " [ 9]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 8]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [11]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 9]\n",
      " [13]\n",
      " [12]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [10]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 1]\n",
      "]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el dataset desde un archivo CSV usando PDL\n",
    "my $filename = './data/pima-indians-diabetes.csv';\n",
    "my $raw_dataset = sml->load_csv($filename);\n",
    "\n",
    "# Convertir datos a formato PDL\n",
    "my $dataset = pdl(map { [map {$_ + 0} @$_] } @$raw_dataset);\n",
    "\n",
    "# Imprimir información del conjunto de datos\n",
    "printf \"Loaded data file %s with %d rows and %d columns.\\n\\n\", $filename, $dataset->dim(0), $dataset->dim(1);\n",
    "print \"Dataset[0]:\\n\", $dataset->slice(\"0,:\"), \"\\n\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15e4418",
   "metadata": {},
   "source": [
    "### 2.2.2 Standardize Data\n",
    "\n",
    "Standardization is a rescaling technique that refers to centering the distribution of the data on\n",
    "the value 0 and the standard deviation to the value 1. Together, the mean and the standard\n",
    "deviation can be used to summarize a normal distribution, also called the Gaussian distribution\n",
    "or bell curve.\n",
    "It requires that the mean and standard deviation of the values for each column be known\n",
    "prior to scaling. As with normalizing above, we can estimate these values from training data, or\n",
    "use domain knowledge to specify their values. Let’s start with creating functions to estimate\n",
    "the mean and standard deviation statistics for each column from a dataset. The mean describes\n",
    "the middle or central tendency for a collection of numbers. The mean for a column is calculated\n",
    "as the sum of all values for a column divided by the total number of values.<br><br>\n",
    "\n",
    "<center>$\\sum_{i=1}^n values_i / count(values)$</center> (2.2)\n",
    "\n",
    "The function below named column_means() calculates the mean values for each column in\n",
    "the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b35534a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "*sml::column_means"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "Warning",
     "evalue": "Subroutine sml::column_means redefined at /usr/local/lib/perl5/site_perl/5.32.1/x86_64-linux/sml.pm line 22.\n",
     "output_type": "error",
     "traceback": [
      "Subroutine sml::column_means redefined at /usr/local/lib/perl5/site_perl/5.32.1/x86_64-linux/sml.pm line 22.\n"
     ]
    }
   ],
   "source": [
    "# Función para calcular la media de cada columna usando PDL\n",
    "sub column_means {\n",
    "    my ($dataset) = @_;\n",
    "\n",
    "    # Transponer el tensor para que cada columna sea una fila temporal\n",
    "    my $transposed = $dataset->xchg(0, 1);\n",
    "\n",
    "    # Calcular la media para cada columna\n",
    "    my $means = $transposed->average;\n",
    "\n",
    "    return $means;\n",
    "}\n",
    "\n",
    "# Registrar el método en la clase\n",
    "sml->add_to_class('column_means', \\&{'column_means'});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771eb60f",
   "metadata": {},
   "source": [
    "The standard deviation describes the average spread of values from the mean. It can be\n",
    "calculated as the square root of the sum of the squared difference between each value and the\n",
    "mean and dividing by the number of values minus 1.<br><br>\n",
    "\n",
    "<center>$ standard\\ deviation = \\sqrt{\\sum_{i=1}^n (values_i - mean)^2 / count(values) − 1}$</center> (2.3)\n",
    "\n",
    "The function below named column stdevs() calculates the standard deviation of values for\n",
    "each column in the dataset and assumes the means have already been calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a85eba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "*sml::column_stdevs"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "Warning",
     "evalue": "Subroutine sml::column_stdevs redefined at /usr/local/lib/perl5/site_perl/5.32.1/x86_64-linux/sml.pm line 22.\n",
     "output_type": "error",
     "traceback": [
      "Subroutine sml::column_stdevs redefined at /usr/local/lib/perl5/site_perl/5.32.1/x86_64-linux/sml.pm line 22.\n"
     ]
    }
   ],
   "source": [
    "# Función para calcular la desviación estándar de cada columna usando PDL\n",
    "sub column_stdevs {\n",
    "    my ($dataset, $means) = @_;\n",
    "\n",
    "    # Transponer el tensor para que cada columna sea una fila temporal\n",
    "    my $transposed = $dataset->xchg(0, 1);\n",
    "\n",
    "    # Ajustar dimensiones de las medias para que coincidan con el dataset\n",
    "    $means = $means->dummy(0);  # Expandir la dimensión para que coincida\n",
    "\n",
    "    # Calcular la varianza para cada columna\n",
    "    my $variance = ($transposed - $means)**2;\n",
    "    my $var_sum = $variance->average;\n",
    "\n",
    "    # Calcular la desviación estándar\n",
    "    my $stdevs = sqrt($var_sum);\n",
    "\n",
    "    return $stdevs;\n",
    "}\n",
    "\n",
    "# Registrar el método en la clase\n",
    "sml->add_to_class('column_stdevs', \\&{'column_stdevs'});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ea1cc5",
   "metadata": {},
   "source": [
    "Using the contrived dataset, we can estimate the summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "765273bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      "\n",
      "[\n",
      " [50 30]\n",
      " [20 90]\n",
      " [30 50]\n",
      "]\n",
      "Means: [33.33, 56.67]\n",
      "Stdevs: [12.47, 24.94]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear un tensor de 3x2 usando PDL\n",
    "my $dataset = pdl [[50, 30], [20, 90], [30, 50]];\n",
    "print \"Dataset:\\n\", $dataset;\n",
    "\n",
    "# Calcular la media y la desviación estándar\n",
    "my $means = column_means($dataset);\n",
    "my $stdevs = column_stdevs($dataset, $means);\n",
    "\n",
    "# Formatear los resultados a dos decimales y convertirlos en cadenas\n",
    "my $formatted_means = join(\", \", map { sprintf \"%0.2f\", $_ } list $means);\n",
    "my $formatted_stdevs = join(\", \", map { sprintf \"%0.2f\", $_ } list $stdevs);\n",
    "\n",
    "# Imprimir resultados\n",
    "print \"Means: [$formatted_means]\\n\";\n",
    "print \"Stdevs: [$formatted_stdevs]\\n\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c456d",
   "metadata": {},
   "source": [
    "Once the summary statistics are calculated, we can easily standardize the values in each\n",
    "column. The calculation to standardize a given value is as follows:<br><br>\n",
    "\n",
    "<center>$standardized\\_value_i = (value_i − mean)\\ /\\ stdev$</center>  (2.4)\n",
    "\n",
    "Below is a function named standardize dataset() that implements this equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52159526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "*sml::standardize_dataset"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "Warning",
     "evalue": "Subroutine sml::standardize_dataset redefined at /usr/local/lib/perl5/site_perl/5.32.1/x86_64-linux/sml.pm line 22.\n",
     "output_type": "error",
     "traceback": [
      "Subroutine sml::standardize_dataset redefined at /usr/local/lib/perl5/site_perl/5.32.1/x86_64-linux/sml.pm line 22.\n"
     ]
    }
   ],
   "source": [
    "# Función para estandarizar un conjunto de datos usando PDL\n",
    "sub standardize_dataset {\n",
    "    my ($dataset, $means, $stdevs) = @_;\n",
    "\n",
    "    # Expandir y repetir las medias y desviaciones estándar para que coincidan con el dataset\n",
    "    $means = $means->dummy(0);  \n",
    "    $stdevs = $stdevs->dummy(0);  \n",
    "\n",
    "    # Ajustar la forma para que coincida con el número de filas del dataset\n",
    "    $means = $means->reshape($dataset->dim(0), $dataset->dim(1));\n",
    "    $stdevs = $stdevs->reshape($dataset->dim(0), $dataset->dim(1));\n",
    "\n",
    "    # Estandarización: (valor - media) / desviación estándar\n",
    "    my $standardized = ($dataset - $means) / $stdevs;\n",
    "\n",
    "    return $standardized;\n",
    "}\n",
    "\n",
    "# Registrar el método en la clase\n",
    "sml->add_to_class('standardize_dataset', \\&{'standardize_dataset'});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b18b1d",
   "metadata": {},
   "source": [
    "Combining this with the functions to estimate the mean and standard deviation summary\n",
    "statistics, we can standardize our contrived dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f697fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file ./data/pima-indians-diabetes.csv with 9 rows and 768 columns.\n",
      "\n",
      "Dataset[0]: \n",
      "[\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 2]\n",
      " [ 8]\n",
      " [ 4]\n",
      " [10]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 7]\n",
      " [ 9]\n",
      " [11]\n",
      " [10]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [13]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [10]\n",
      " [ 4]\n",
      " [11]\n",
      " [ 9]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 9]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 5]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [13]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 5]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [13]\n",
      " [ 2]\n",
      " [15]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 9]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 7]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [17]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 6]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 8]\n",
      " [ 6]\n",
      " [ 0]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 9]\n",
      " [ 7]\n",
      " [11]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 9]\n",
      " [12]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 9]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 9]\n",
      " [10]\n",
      " [ 0]\n",
      " [ 9]\n",
      " [ 1]\n",
      " [ 9]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [12]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [11]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [13]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [10]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [14]\n",
      " [ 8]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 7]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [13]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [ 8]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [12]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 5]\n",
      " [ 9]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 9]\n",
      " [ 1]\n",
      " [13]\n",
      " [12]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [12]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 9]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 8]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [12]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 8]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [14]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 8]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 4]\n",
      " [ 8]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 8]\n",
      " [12]\n",
      " [ 0]\n",
      " [ 9]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 9]\n",
      " [ 7]\n",
      " [13]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 9]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [11]\n",
      " [11]\n",
      " [ 6]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 6]\n",
      " [12]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 6]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [11]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 7]\n",
      " [ 6]\n",
      " [11]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 9]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [10]\n",
      " [13]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [11]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [11]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [10]\n",
      " [ 6]\n",
      " [ 9]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 6]\n",
      " [ 9]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [13]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [10]\n",
      " [ 2]\n",
      " [ 9]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [ 8]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [11]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 9]\n",
      " [13]\n",
      " [12]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [10]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 1]\n",
      "]\n",
      "\n",
      "\n",
      "Dataset[0]: (NaN, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, Inf, Inf, NaN, Inf, Inf, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, NaN, Inf, Inf, Inf, NaN, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, NaN, Inf, NaN, NaN, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, NaN, Inf, Inf, Inf, NaN, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, NaN, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, Inf, NaN, Inf, Inf, Inf, Inf, NaN, NaN, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, NaN, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, Inf, Inf, NaN, NaN, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el dataset desde un archivo CSV usando PDL\n",
    "my $filename = './data/pima-indians-diabetes.csv';\n",
    "my $raw_dataset = sml->load_csv($filename);\n",
    "\n",
    "# Convertir datos a formato PDL, asegurando que los valores sean flotantes\n",
    "my $dataset = pdl(map { [map {$_ + 0} @$_] } @$raw_dataset);\n",
    "print \"Loaded data file $filename with \", $dataset->dim(0), \" rows and \", $dataset->dim(1), \" columns.\\n\\n\";\n",
    "\n",
    "# Ver el primer registro del dataset original\n",
    "print \"Dataset[0]: \", $dataset->slice(\"0,:\"), \"\\n\\n\";\n",
    "\n",
    "# Calcular el mínimo y el máximo para cada columna\n",
    "my $minmax = dataset_minmax($dataset);\n",
    "\n",
    "# Normalizar el conjunto de datos\n",
    "$dataset = normalize_dataset($dataset, $minmax);\n",
    "\n",
    "# Calcular la media y la desviación estándar\n",
    "my $means = column_means($dataset);\n",
    "my $stdevs = column_stdevs($dataset, $means);\n",
    "\n",
    "# Estandarizar el conjunto de datos\n",
    "$dataset = standardize_dataset($dataset, $means, $stdevs);\n",
    "\n",
    "# Imprimir el primer registro del dataset estandarizado\n",
    "my $formatted_record = join(\", \", map { sprintf \"%0.2f\", $_ } list $dataset->slice(\"0,:\"));\n",
    "print \"Dataset[0]: ($formatted_record)\\n\\n\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6d110e",
   "metadata": {},
   "source": [
    "Again, we can demonstrate the standardization of a machine learning dataset. The example\n",
    "below demonstrates how to load and standardize the Pima Indians diabetes dataset, assumed\n",
    "to be in the current working directory as in the previous normalization example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eca13e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file ./data/pima-indians-diabetes.csv with 9 rows and 768 columns.\n",
      "\n",
      "Dataset[0]: 6.00, 1.00, 8.00, 1.00, 0.00, 5.00, 3.00, 10.00, 2.00, 8.00, 4.00, 10.00, 10.00, 1.00, 5.00, 7.00, 0.00, 7.00, 1.00, 1.00, 3.00, 8.00, 7.00, 9.00, 11.00, 10.00, 7.00, 1.00, 13.00, 5.00, 5.00, 3.00, 3.00, 6.00, 10.00, 4.00, 11.00, 9.00, 2.00, 4.00, 3.00, 7.00, 7.00, 9.00, 7.00, 0.00, 1.00, 2.00, 7.00, 7.00, 1.00, 1.00, 5.00, 8.00, 7.00, 1.00, 7.00, 0.00, 0.00, 0.00, 2.00, 8.00, 5.00, 2.00, 7.00, 5.00, 0.00, 2.00, 1.00, 4.00, 2.00, 5.00, 13.00, 4.00, 1.00, 1.00, 7.00, 5.00, 0.00, 2.00, 3.00, 2.00, 7.00, 0.00, 5.00, 2.00, 13.00, 2.00, 15.00, 1.00, 1.00, 4.00, 7.00, 4.00, 2.00, 6.00, 2.00, 1.00, 6.00, 1.00, 1.00, 1.00, 0.00, 1.00, 2.00, 1.00, 1.00, 4.00, 3.00, 0.00, 3.00, 8.00, 1.00, 4.00, 7.00, 4.00, 5.00, 5.00, 4.00, 4.00, 0.00, 6.00, 2.00, 5.00, 0.00, 1.00, 3.00, 1.00, 1.00, 0.00, 4.00, 9.00, 3.00, 8.00, 2.00, 2.00, 0.00, 0.00, 0.00, 5.00, 3.00, 5.00, 2.00, 10.00, 4.00, 0.00, 9.00, 2.00, 5.00, 2.00, 1.00, 4.00, 9.00, 1.00, 8.00, 7.00, 2.00, 1.00, 2.00, 17.00, 4.00, 7.00, 0.00, 2.00, 0.00, 6.00, 3.00, 4.00, 4.00, 3.00, 6.00, 6.00, 2.00, 1.00, 2.00, 8.00, 6.00, 0.00, 5.00, 5.00, 6.00, 0.00, 1.00, 5.00, 4.00, 7.00, 8.00, 1.00, 8.00, 5.00, 3.00, 9.00, 7.00, 11.00, 8.00, 5.00, 1.00, 3.00, 4.00, 4.00, 0.00, 1.00, 0.00, 2.00, 6.00, 5.00, 8.00, 5.00, 1.00, 7.00, 2.00, 0.00, 7.00, 0.00, 9.00, 12.00, 5.00, 6.00, 5.00, 5.00, 0.00, 2.00, 7.00, 7.00, 1.00, 1.00, 0.00, 3.00, 4.00, 0.00, 4.00, 6.00, 1.00, 4.00, 3.00, 4.00, 7.00, 0.00, 9.00, 0.00, 1.00, 4.00, 3.00, 6.00, 2.00, 9.00, 10.00, 0.00, 9.00, 1.00, 9.00, 2.00, 2.00, 0.00, 12.00, 1.00, 3.00, 2.00, 1.00, 11.00, 3.00, 3.00, 4.00, 3.00, 4.00, 5.00, 0.00, 2.00, 0.00, 2.00, 10.00, 2.00, 3.00, 1.00, 13.00, 2.00, 7.00, 0.00, 5.00, 2.00, 0.00, 10.00, 7.00, 7.00, 2.00, 7.00, 5.00, 1.00, 4.00, 5.00, 0.00, 0.00, 2.00, 1.00, 0.00, 6.00, 2.00, 0.00, 14.00, 8.00, 0.00, 2.00, 5.00, 5.00, 3.00, 2.00, 10.00, 0.00, 0.00, 2.00, 6.00, 0.00, 2.00, 3.00, 7.00, 2.00, 3.00, 3.00, 3.00, 6.00, 4.00, 3.00, 0.00, 13.00, 2.00, 1.00, 1.00, 10.00, 2.00, 6.00, 8.00, 2.00, 1.00, 12.00, 1.00, 0.00, 0.00, 5.00, 9.00, 7.00, 1.00, 1.00, 1.00, 5.00, 8.00, 8.00, 1.00, 3.00, 3.00, 5.00, 4.00, 4.00, 3.00, 1.00, 3.00, 9.00, 1.00, 13.00, 12.00, 1.00, 5.00, 5.00, 5.00, 4.00, 4.00, 5.00, 6.00, 0.00, 3.00, 1.00, 3.00, 0.00, 0.00, 2.00, 2.00, 12.00, 0.00, 1.00, 4.00, 0.00, 1.00, 0.00, 1.00, 1.00, 1.00, 1.00, 5.00, 8.00, 5.00, 3.00, 1.00, 5.00, 1.00, 4.00, 4.00, 2.00, 3.00, 0.00, 3.00, 3.00, 4.00, 6.00, 5.00, 9.00, 5.00, 2.00, 4.00, 0.00, 8.00, 1.00, 6.00, 1.00, 1.00, 1.00, 0.00, 3.00, 1.00, 4.00, 1.00, 3.00, 1.00, 2.00, 0.00, 2.00, 8.00, 4.00, 0.00, 1.00, 0.00, 1.00, 2.00, 3.00, 1.00, 2.00, 1.00, 0.00, 12.00, 5.00, 1.00, 6.00, 0.00, 2.00, 4.00, 8.00, 4.00, 0.00, 1.00, 0.00, 0.00, 0.00, 1.00, 2.00, 0.00, 2.00, 2.00, 14.00, 1.00, 5.00, 10.00, 9.00, 9.00, 1.00, 8.00, 5.00, 10.00, 0.00, 0.00, 0.00, 8.00, 6.00, 1.00, 0.00, 0.00, 7.00, 4.00, 0.00, 2.00, 7.00, 8.00, 4.00, 3.00, 0.00, 4.00, 0.00, 0.00, 0.00, 1.00, 0.00, 4.00, 8.00, 2.00, 2.00, 4.00, 4.00, 3.00, 6.00, 5.00, 2.00, 7.00, 6.00, 2.00, 3.00, 6.00, 7.00, 3.00, 10.00, 0.00, 1.00, 2.00, 8.00, 12.00, 0.00, 9.00, 2.00, 3.00, 3.00, 9.00, 7.00, 13.00, 6.00, 2.00, 3.00, 6.00, 9.00, 3.00, 3.00, 1.00, 3.00, 0.00, 0.00, 2.00, 0.00, 1.00, 6.00, 1.00, 4.00, 0.00, 0.00, 0.00, 3.00, 8.00, 3.00, 10.00, 4.00, 1.00, 8.00, 5.00, 4.00, 1.00, 4.00, 1.00, 3.00, 6.00, 1.00, 1.00, 7.00, 1.00, 8.00, 11.00, 11.00, 6.00, 0.00, 1.00, 6.00, 0.00, 2.00, 1.00, 6.00, 4.00, 0.00, 3.00, 2.00, 3.00, 2.00, 1.00, 1.00, 6.00, 2.00, 10.00, 2.00, 0.00, 6.00, 12.00, 8.00, 8.00, 1.00, 8.00, 6.00, 3.00, 0.00, 11.00, 2.00, 3.00, 2.00, 6.00, 0.00, 0.00, 1.00, 1.00, 1.00, 1.00, 6.00, 1.00, 7.00, 4.00, 1.00, 1.00, 1.00, 0.00, 1.00, 3.00, 3.00, 7.00, 6.00, 11.00, 3.00, 6.00, 2.00, 9.00, 0.00, 2.00, 2.00, 6.00, 0.00, 2.00, 4.00, 0.00, 0.00, 5.00, 4.00, 7.00, 0.00, 2.00, 1.00, 10.00, 13.00, 5.00, 2.00, 7.00, 1.00, 0.00, 4.00, 6.00, 4.00, 3.00, 2.00, 1.00, 0.00, 11.00, 0.00, 1.00, 1.00, 5.00, 2.00, 1.00, 2.00, 2.00, 1.00, 11.00, 3.00, 10.00, 1.00, 8.00, 9.00, 6.00, 1.00, 4.00, 10.00, 6.00, 9.00, 6.00, 1.00, 10.00, 3.00, 8.00, 6.00, 9.00, 0.00, 3.00, 2.00, 2.00, 0.00, 0.00, 4.00, 5.00, 2.00, 3.00, 1.00, 1.00, 1.00, 8.00, 13.00, 2.00, 7.00, 2.00, 7.00, 3.00, 0.00, 4.00, 4.00, 2.00, 6.00, 1.00, 2.00, 4.00, 6.00, 10.00, 2.00, 9.00, 2.00, 3.00, 5.00, 10.00, 0.00, 3.00, 7.00, 3.00, 10.00, 1.00, 5.00, 4.00, 1.00, 1.00, 5.00, 1.00, 4.00, 1.00, 0.00, 2.00, 2.00, 3.00, 8.00, 2.00, 2.00, 2.00, 4.00, 0.00, 8.00, 2.00, 1.00, 11.00, 3.00, 1.00, 9.00, 13.00, 12.00, 1.00, 1.00, 3.00, 6.00, 4.00, 1.00, 3.00, 0.00, 8.00, 1.00, 7.00, 0.00, 1.00, 6.00, 2.00, 9.00, 9.00, 10.00, 2.00, 5.00, 1.00, 1.00\n",
      "\n",
      "Minimos y Maximos por columna:\n",
      "\n",
      "[\n",
      " [    0    17]\n",
      " [    0   199]\n",
      " [    0   122]\n",
      " [    0    99]\n",
      " [    0   846]\n",
      " [    0  67.1]\n",
      " [0.078  2.42]\n",
      " [   21    81]\n",
      " [    0     1]\n",
      "]\n",
      "\n",
      "Dataset Normalizado[0]: Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, Inf, Inf, NaN, Inf, Inf, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, NaN, Inf, Inf, Inf, NaN, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, NaN, Inf, NaN, NaN, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, NaN, Inf, Inf, Inf, NaN, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, NaN, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, Inf, NaN, Inf, Inf, Inf, Inf, NaN, NaN, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, NaN, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, Inf, Inf, NaN, NaN, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf\n",
      "\n",
      "Medias: NaN, NaN, NaN, NaN, NaN, NaN, Inf, Inf, NaN\n",
      "Desviaciones: NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN\n",
      "\n",
      "Dataset Estandarizado[0]: NaN, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, Inf, Inf, NaN, Inf, Inf, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, NaN, Inf, Inf, Inf, NaN, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, NaN, Inf, NaN, NaN, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, NaN, Inf, Inf, Inf, NaN, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, NaN, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, NaN, Inf, NaN, Inf, Inf, Inf, Inf, NaN, NaN, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, NaN, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, NaN, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, Inf, Inf, NaN, NaN, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, NaN, Inf, Inf, Inf, NaN, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el dataset desde un archivo CSV usando PDL\n",
    "my $filename = './data/pima-indians-diabetes.csv';\n",
    "my $raw_dataset = sml->load_csv($filename);\n",
    "\n",
    "# Convertir datos a formato PDL, asegurando que los valores sean flotantes\n",
    "my $dataset = pdl(map { [map {$_ + 0} @$_] } @$raw_dataset);\n",
    "print \"Loaded data file $filename with \", $dataset->dim(0), \" rows and \", $dataset->dim(1), \" columns.\\n\\n\";\n",
    "\n",
    "# Ver el primer registro del dataset original\n",
    "print \"Dataset[0]: \", join(\", \", map { sprintf \"%0.2f\", $_ } list $dataset->slice(\"0,:\")), \"\\n\\n\";\n",
    "\n",
    "# Calcular el mínimo y el máximo para cada columna\n",
    "my $minmax = dataset_minmax($dataset);\n",
    "print \"Minimos y Maximos por columna:\\n\", $minmax, \"\\n\";\n",
    "\n",
    "# Normalizar el conjunto de datos\n",
    "$dataset = normalize_dataset($dataset, $minmax);\n",
    "print \"Dataset Normalizado[0]: \", join(\", \", map { sprintf \"%0.2f\", $_ } list $dataset->slice(\"0,:\")), \"\\n\\n\";\n",
    "\n",
    "# Calcular la media y la desviación estándar\n",
    "my $means = column_means($dataset);\n",
    "my $stdevs = column_stdevs($dataset, $means);\n",
    "print \"Medias: \", join(\", \", map { sprintf \"%0.2f\", $_ } list $means), \"\\n\";\n",
    "print \"Desviaciones: \", join(\", \", map { sprintf \"%0.2f\", $_ } list $stdevs), \"\\n\\n\";\n",
    "\n",
    "# Estandarizar el conjunto de datos\n",
    "$dataset = standardize_dataset($dataset, $means, $stdevs);\n",
    "print \"Dataset Estandarizado[0]: \", join(\", \", map { sprintf \"%0.2f\", $_ } list $dataset->slice(\"0,:\")), \"\\n\\n\";\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b83fbd",
   "metadata": {},
   "source": [
    "### 2.2.3 When to Normalize and Standardize\n",
    "\n",
    "Standardization is a scaling technique that assumes your data conforms to a normal distribution.\n",
    "If a given data attribute is normal or close to normal, this is probably the scaling method to use.\n",
    "It is good practice to record the summary statistics used in the standardization process so that\n",
    "you can apply them when standardizing data in the future that you may want to use with your\n",
    "model. Normalization is a scaling technique that does not assume any specific distribution.\n",
    "\n",
    "If your data is not normally distributed, consider normalizing it prior to applying your\n",
    "machine learning algorithm. It is good practice to record the minimum and maximum values\n",
    "for each column used in the normalization process, again, in case you need to normalize new\n",
    "data in the future to be used with your model.\n",
    "\n",
    "## 2.3 Extensions\n",
    "\n",
    "There are many other data transforms you could apply. The idea of data transforms is to best\n",
    "expose the structure of your problem in your data to the learning algorithm. It may not be\n",
    "clear what transforms are required upfront. A combination of trial and error and exploratory\n",
    "data analysis (plots and stats) can help tease out what may work. Below are some additional\n",
    "transforms you may want to consider researching and implementing:\n",
    "* Normalization that permits a configurable range, such as -1 to 1 and more.\n",
    "* Standardization that permits a configurable spread, such as 1, 2 or more standard deviations\n",
    "from the mean.\n",
    "* Exponential transforms such as logarithm, square root and exponents.\n",
    "* Power transforms such as Box-Cox for fixing the skew in normally distributed data.\n",
    "\n",
    "## 2.4 Review\n",
    "\n",
    "In this tutorial, you discovered how to rescale your data for machine learning from scratch.\n",
    "Specifically, you learned:\n",
    "* How to normalize data from scratch.\n",
    "* How to standardize data from scratch.\n",
    "* When to use normalization or standardization on your data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPerl 0.011",
   "language": "perl",
   "name": "iperl"
  },
  "language_info": {
   "file_extension": ".pl",
   "mimetype": "text/x-perl",
   "name": "perl",
   "version": "5.32.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
