{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad02463d",
   "metadata": {},
   "source": [
    "# Chapter 4 Evaluation Metrics\n",
    "\n",
    "After you make predictions, you need to know if they are any good. There are standard measures\n",
    "that we can use to summarize how good a set of predictions actually is. Knowing how good a set\n",
    "of predictions is allows you to make estimates about the skill of a given machine learning model\n",
    "of your problem. In this tutorial, you will discover how to implement four standard prediction\n",
    "evaluation metrics from scratch in Perl.\n",
    "After reading this tutorial, you will know:\n",
    "\n",
    "* How to implement classification accuracy.\n",
    "* How to implement and interpret a confusion matrix.\n",
    "* How to implement mean absolute error for regression.\n",
    "* How to implement root mean squared error for regression.\n",
    "\n",
    "Let’s get started.\n",
    "\n",
    "## 4.1 Description\n",
    "\n",
    "You must estimate the quality of a set of predictions when training a machine learning model.\n",
    "Performance metrics like classification accuracy and root mean squared error can give you a\n",
    "clear objective idea of how good a set of predictions is, and in turn how good the model is that\n",
    "generated them. This is important as it allows you to tell the difference and select among:\n",
    "\n",
    "* Different transforms of the data used to train the same machine learning model.\n",
    "* Different machine learning models trained on the same data.\n",
    "* Different configurations for a machine learning model trained on the same data.\n",
    "\n",
    "As such, performance metrics are a required building block in implementing machine learning\n",
    "algorithms from scratch.\n",
    "\n",
    "## 4.2. Tutorial\n",
    "\n",
    "This tutorial is divided into 4 parts:\n",
    "\n",
    "1. Classification Accuracy.\n",
    "2. Confusion Matrix.\n",
    "3. Mean Absolute Error.\n",
    "4. Root Mean Squared Error.\n",
    "\n",
    "These steps will provide the foundations you need to handle evaluating predictions made by\n",
    "machine learning algorithms.\n",
    "\n",
    "### 4.2.1 Classification Accuracy\n",
    "\n",
    "A quick way to evaluate a set of predictions on a classification problem is by using accuracy.\n",
    "Classification accuracy is a ratio of the number of correct predictions out of all predictions that\n",
    "were made. It is often presented as a percentage between 0% for the worst possible accuracy\n",
    "and 100% for the best possible accuracy.\n",
    "\n",
    "<center>accuracy = correct predictions x 100 / total predictions</center>\n",
    "(4.1)\n",
    "\n",
    "We can implement this in a function that takes the expected outcomes and the predictions\n",
    "as arguments. Below is this function named accuracy_metric() that returns classification\n",
    "accuracy as a percentage. Notice that we use == to compare the equality actual to predicted\n",
    "values. This allows us to compare integers or strings, two main data types that we may choose\n",
    "to use when loading classification data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e511cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "use strict;\n",
    "use warnings;\n",
    "use Data::Dump qw(dump) ;\n",
    "use List::Util qw(zip min max sum uniq);\n",
    "use sml; #Statistical Machine Learning Library\n",
    "use AI::MXNet qw(mx);\n",
    "IPerl->load_plugin('Chart::Plotly');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a4b8dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CODE(0xa443700)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defined in Section 4.2.1 Classification Accuracy\n",
    "# Function To Calculate Classification Accuracy.\n",
    "# Calculate accuracy percentage between two lists\n",
    "my $accuracy_metric = sub {\n",
    "    my ($self, $actual, $predicted) = @_;\n",
    "    my $actual_nd    = mx->nd->array($actual);\n",
    "    my $predicted_nd = mx->nd->array($predicted);\n",
    "    my $correct_nd   = mx->nd->equal($actual_nd, $predicted_nd);\n",
    "    my $correct      = $correct_nd->sum->asscalar;\n",
    "    return sprintf '%.1f', ($correct / @$actual) * 100.0;\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe441e8",
   "metadata": {},
   "source": [
    "We can contrive a small dataset to test this function. Below are a set of 10 actual and\n",
    "predicted integer values. There are two mistakes in the set of predictions.\n",
    "\n",
    "Running this example produces the expected accuracy of 80% or 8/10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "348d9a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual\tpredicted\n",
      "0\t0\n",
      "0\t1\n",
      "0\t0\n",
      "0\t0\n",
      "0\t0\n",
      "1\t1\n",
      "1\t0\n",
      "1\t1\n",
      "1\t1\n",
      "1\t1\n",
      "80.0"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of calculating classification accuracy\n",
    "my $actual    = [0,0,0,0,0,1,1,1,1,1];\n",
    "my $predicted = [0,1,0,0,0,1,0,1,1,1];\n",
    "print \"actual\\tpredicted\\n\";\n",
    "print $_->[0], \"\\t\", $_->[1], \"\\n\" for zip ($actual, $predicted);\n",
    "my $accuracy = sml->accuracy_metric($actual, $predicted);\n",
    "print $accuracy;\n",
    "\n",
    "# Example Output From Calculating Classification Accuracy.\n",
    "# actual\tpredicted\n",
    "# 0\t0\n",
    "# 0\t1\n",
    "# 0\t0\n",
    "# 0\t0\n",
    "# 0\t0\n",
    "# 1\t1\n",
    "# 1\t0\n",
    "# 1\t1\n",
    "# 1\t1\n",
    "# 1\t1\n",
    "# 80.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb569df",
   "metadata": {},
   "source": [
    "Accuracy is a good metric to use when you have a small number of class values, such as 2,\n",
    "also called a binary classification problem. Accuracy starts to lose it’s meaning when you have more class values and you may need to review a different perspective on the results, such as a confusion matrix.\n",
    "\n",
    "### 4.2.2 Confusion Matrix\n",
    "\n",
    "A confusion matrix provides a summary of all of the predictions made compared to the expected actual values. The results are presented in a matrix with counts in each cell. The counts of predicted class values are summarized horizontally (rows), whereas the counts of actual values for each class values are presented vertically (columns). A perfect set of predictions is shown as\n",
    "a diagonal line from the top left to the bottom right of the matrix.\n",
    "The value of a confusion matrix for classification problems is that you can clearly see which\n",
    "predictions were wrong and the type of mistake that was made. Let’s create a function to\n",
    "calculate a confusion matrix.\n",
    "We can start off by defining the function to calculate the confusion matrix given a list\n",
    "of actual class values and a list of predictions. The function is listed below and is named\n",
    "confusion matrix(). It first makes a list of all of the unique class values and assigns each class\n",
    "value a unique integer or index into the confusion matrix.\n",
    "The confusion matrix is always square, with the number of class values indicating the number\n",
    "of rows and columns required. Here, the first index into the matrix is the row for actual values\n",
    "and the second is the column for predicted values. After the square confusion matrix is created\n",
    "and initialized to zero counts in each cell, it is a matter of looping through all predictions and incrementing the count in each cell. The function returns two objects. The first is the set of\n",
    "unique class values, so that they can be displayed when the confusion matrix is drawn. The\n",
    "second is the confusion matrix itself with the counts in each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b653bf49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "*sml::confusion_matrix"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of Calculating and Displaying a Pretty Confusion Matrix\n",
    "my $confusion_matrix = sub {\n",
    "    my ($self, $actual_ref, $predicted_ref) = @_;\n",
    "\n",
    "    # Convertimos a tensores\n",
    "    my $actual_nd    = mx->nd->array($actual_ref);\n",
    "    my $predicted_nd = mx->nd->array($predicted_ref);\n",
    "\n",
    "    # Extraer datos como arrays regulares para uniq y lookup\n",
    "    my @actual    = @$actual_ref;\n",
    "    my @predicted = @$predicted_ref;\n",
    "    my @unique    = uniq @actual;\n",
    "\n",
    "    # Mapear clases a índices\n",
    "    my (%lookup, $x, $y);\n",
    "    while (my ($i, $val) = each @unique) {\n",
    "        $lookup{$val} = $i;\n",
    "    }\n",
    "\n",
    "    # Inicializar matriz de confusión con tensores\n",
    "    my $num_classes = scalar @unique;\n",
    "    my $confusion = mx->nd->zeros([$num_classes, $num_classes]);\n",
    "\n",
    "    # Llenar la matriz\n",
    "    for my $i (0 .. $#actual) {\n",
    "        $x = $lookup{$actual[$i]};\n",
    "        $y = $lookup{$predicted[$i]};\n",
    "        my $current_value = $confusion->at([$x, $y])->asscalar;\n",
    "        $confusion->at([$x, $y]) .= $current_value + 1;\n",
    "    }\n",
    "\n",
    "    return \\@unique, $confusion;\n",
    "};\n",
    "\n",
    "sml->add_to_class('sml', 'confusion_matrix', $confusion_matrix);\n",
    "# Function To Calculate a Confusion Matrix.\n",
    "# calculate a confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d0fa6c",
   "metadata": {},
   "source": [
    "Let’s make this concrete with an example. Below is another contrived dataset, this time\n",
    "with 3 mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98c71ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual\tpredicted\n",
      "0\t0\n",
      "0\t1\n",
      "0\t1\n",
      "0\t0\n",
      "1\t0\n",
      "1\t1\n",
      "1\t1\n",
      "1\t1\n"
     ]
    }
   ],
   "source": [
    "# Example of a Set of Contrived Predictions and Expected Values.\n",
    "my $actual    = [0,0,0,0,1,1,1,1];\n",
    "my $predicted = [0,1,1,0,0,1,1,1];\n",
    "print \"actual\\tpredicted\\n\";\n",
    "print $_->[0], \"\\t\", $_->[1], \"\\n\" for zip ($actual, $predicted);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e5cd4a",
   "metadata": {},
   "source": [
    "We can calculate and print the confusion matrix for this dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c76b6036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "[[2, 2], [1, 3]]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test confusion matrix with integers\n",
    "my ($unique, $matrix) = sml->confusion_matrix($actual, $predicted);\n",
    "print dump $unique;\n",
    "print \"\\n\", dump $matrix;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289f6ac3",
   "metadata": {},
   "source": [
    "It’s hard to interpret the results this way. It would help if we could display the matrix as\n",
    "intended with rows and columns. Below is a function to correctly display the matrix. The\n",
    "function is named print confusion matrix(). It names the columns as Z for Actual and\n",
    "the rows as P for Predicted. Each column and row are named for the class value to which it\n",
    "corresponds.\n",
    "The matrix is laid out with the expectation that each class label is a single character or\n",
    "single digit integer and that the counts are also single digit integers. You could extend it to\n",
    "handle large class labels or prediction counts as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c33997e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "*sml::print_confusion_matrix"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function To Pretty Print a Confusion Matrix.\n",
    "# pretty print a confusion matrix\n",
    "my $print_confusion_matrix = sub{\n",
    "    my ($self, $unique, $matrix) = @_;\n",
    "    print 'A/P ', join(' ', map {$_} @$unique), \"\\n\";\n",
    "    while (my ($i, $x) = each @$unique){\n",
    "        print sprintf \" %s| %s\\n\", $x, join(' ', map {$_} @{$matrix->[$i]});\n",
    "    }\n",
    "};\n",
    "\n",
    "sml->add_to_class('sml', 'print_confusion_matrix', $print_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b98f771",
   "metadata": {},
   "source": [
    "Running the example produces the output below. We can see the class labels of 0 and 1\n",
    "across the top and bottom. Looking down the diagonal of the matrix from the top left to bottom\n",
    "right, we can see that 3 predictions of 0 were correct and 4 predictions of 1 were correct.\n",
    "Looking in the other cells, we can see 2 + 1 or 3 prediction errors. We can see that 2\n",
    "predictions were made as a 1 that were in fact actually a 0 class value. And we can see 1\n",
    "prediction that was a 0 that was in fact actually a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dde6a206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A/P 0 1\n",
      " 0| 2 2\n",
      " 1| 1 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Output From Printing a Pretty Confusion Matrix.\n",
    "# Test confusion matrix with integers\n",
    "my $actual    = [0,0,0,0,1,1,1,1];\n",
    "my $predicted = [0,1,1,0,0,1,1,1];\n",
    "my ($unique, $matrix) = sml->confusion_matrix($actual, $predicted);\n",
    "sml->print_confusion_matrix($unique, $matrix);\n",
    "\n",
    "# (A)0 1 # Example Output From Printing a Pretty Confusion Matrix.\n",
    "# 0| 3 2\n",
    "# 1| 1 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1a7a8",
   "metadata": {},
   "source": [
    "A confusion matrix is always a good idea to use in addition to classification accuracy to help\n",
    "interpret the predictions.\n",
    "\n",
    "### 4.2.3 Mean Absolute Error\n",
    "\n",
    "Regression problems are those where a real value is predicted. An easy metric to consider is the\n",
    "error in the predicted values as compared to the expected values. The Mean Absolute Error or\n",
    "MAE for short is a good first error metric to use. It is calculated as the average of the absolute\n",
    "error values, where absolute means made positive so that they can be added together.\n",
    "\n",
    "<center>$MAE = \\sum_{i=1}^n abs(predicted_i - actual_i)\\ /\\ total predictions$</center> (4.2)\n",
    "\n",
    "Below is a function named mae metric() that implements this metric. As above, it expects\n",
    "a list of actual outcome values and a list of predictions. We use the built-in abs() Python\n",
    "function to calculate the absolute error values that are summed together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2daf15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "*sml::mae_metric"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function To Calculate Mean Absolute Error.\n",
    "# Calculate mean absolute error\n",
    "my $mae_metric = sub {\n",
    "    my ($self, $actual, $predicted) = @_;\n",
    "    my $actual_nd    = mx->nd->array($actual);\n",
    "    my $predicted_nd = mx->nd->array($predicted);\n",
    "    my $abs_diff     = mx->nd->abs($actual_nd - $predicted_nd);\n",
    "    my $mae          = $abs_diff->mean->asscalar;\n",
    "    return sprintf '%.3f', $mae;\n",
    "};\n",
    "sml->add_to_class('sml', 'mae_metric', $mae_metric);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd50a40",
   "metadata": {},
   "source": [
    "We can contrive a small regression dataset to test this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1106b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual\tpredicted\n",
      "0.1\t0.11\n",
      "0.2\t0.19\n",
      "0.3\t0.29\n",
      "0.4\t0.41\n",
      "0.5\t0.5\n"
     ]
    }
   ],
   "source": [
    "# Small Set of Contrived Regression Predictions and Actual Values.\n",
    "$actual    = [0.1, 0.2, 0.3, 0.4, 0.5];\n",
    "$predicted = [0.11, 0.19, 0.29, 0.41, 0.5];\n",
    "print \"actual\\tpredicted\\n\";\n",
    "print $_->[0], \"\\t\", $_->[1], \"\\n\" for zip ($actual, $predicted);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06b28f0",
   "metadata": {},
   "source": [
    "Only one prediction (0.5) is correct, whereas all other predictions are wrong by 0.01. Therefore, we would expect the mean absolute error (or the average positive error) for these predictions to be a little less than 0.01. Below is an example that tests the mae metric() function with the contrived dataset.\n",
    "\n",
    "Running this example prints the output below. We can see that as expected, the MAE was\n",
    "0.008, a small value slightly lower than 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52250397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test MAE\n",
    "my $mae = sml->mae_metric($actual, $predicted);\n",
    "print $mae;\n",
    "\n",
    "# Example Output From Calculating the Mean Absolute Error.\n",
    "# 0.008"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017eae9c",
   "metadata": {},
   "source": [
    "### 4.2.4 Root Mean Squared Error\n",
    "\n",
    "Another popular way to calculate the error in a set of regression predictions is to use the Root Mean Squared Error. Shortened as RMSE, the metric is sometimes called Mean Squared Error or MSE, dropping the Root part from the calculation and the name. RMSE is calculated as the square root of the mean of the squared differences between actual outcomes and predictions.\n",
    "Squaring each error forces the values to be positive, and the square root of the mean squared error returns the error metric back to the original units for comparison.<br><br>\n",
    "\n",
    "<center>$RMSE = \\sqrt(\\sum_{i=1}^n (predicted_i - actual_i)^2\\ /\\ total predictions)$</center> (4.3)\n",
    "\n",
    "Below is an implementation of this in a function named rmse metric(). It uses the sqrt()\n",
    "function from the math module and uses the ** operator to raise the error to the 2nd power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92215557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "*sml::rmse_metric"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defined in Section 4.2.4 Root Mean Squared Error\n",
    "# Function To Calculate Root Mean Squared Error.\n",
    "# Calculate root mean squared error\n",
    "my $rmse_metric = sub {\n",
    "    my ($self, $actual, $predicted) = @_;\n",
    "    my $actual_nd    = mx->nd->array($actual);\n",
    "    my $predicted_nd = mx->nd->array($predicted);\n",
    "    my $sq_diff      = ($actual_nd - $predicted_nd) ** 2;\n",
    "    my $mean_sq      = $sq_diff->mean->asscalar;\n",
    "    return sprintf '%.4f', sqrt($mean_sq);\n",
    "};\n",
    "\n",
    "sml->add_to_class('sml', 'rmse_metric', $rmse_metric);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d0e76",
   "metadata": {},
   "source": [
    "We can test this metric on the same dataset used to test the calculation of Mean Absolute\n",
    "Error above. Below is a complete example. Again, we would expect an error value to be\n",
    "generally close to 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c803ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0089"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test RMSE\n",
    "$actual    = [0.1, 0.2, 0.3, 0.4, 0.5];\n",
    "$predicted = [0.11, 0.19, 0.29, 0.41, 0.5];\n",
    "my $rmse = sml->rmse_metric($actual, $predicted);\n",
    "print $rmse;\n",
    "# Example Output From Calculating the Root Mean Squared Error.\n",
    "# 0.0089"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69a7af",
   "metadata": {},
   "source": [
    "### 4.2.5 ROC curves\n",
    "\n",
    "Computing AUC ROC from scratch in Perl without using any libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39d16493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "*sml::perf_metrics"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to calculate the ROC metrics\n",
    "my $perf_metrics = sub {\n",
    "    my ($self, $actual_ref, $y_hat_ref, $threshold) = @_;\n",
    "\n",
    "    # Convertir a tensores\n",
    "    my $actual = mx->nd->array($actual_ref);\n",
    "    my $y_hat  = mx->nd->array($y_hat_ref);\n",
    "\n",
    "    # Aplicar el umbral: 1 si <= threshold, 0 en caso contrario (predicción positiva)\n",
    "    my $predicted_positive = mx->nd->less_equal($y_hat, $threshold)->astype('float32');\n",
    "    my $predicted_negative = mx->nd->greater($y_hat, $threshold)->astype('float32');\n",
    "\n",
    "    # Verdaderos positivos: actual=1 y predicho=1\n",
    "    my $tp = mx->nd->sum($predicted_positive * mx->nd->equal($actual, 1))->asscalar;\n",
    "\n",
    "    # Falsos positivos: actual=0 y predicho=1\n",
    "    my $fp = mx->nd->sum($predicted_positive * mx->nd->equal($actual, 0))->asscalar;\n",
    "\n",
    "    # Verdaderos negativos: actual=0 y predicho=0\n",
    "    my $tn = mx->nd->sum($predicted_negative * mx->nd->equal($actual, 0))->asscalar;\n",
    "\n",
    "    # Falsos negativos: actual=1 y predicho=0\n",
    "    my $fn = mx->nd->sum($predicted_negative * mx->nd->equal($actual, 1))->asscalar;\n",
    "\n",
    "    # Tasa de verdaderos positivos (sensibilidad)\n",
    "    my $tpr = ($tp + $fn) > 0 ? $tp / ($tp + $fn) : 0;\n",
    "\n",
    "    # Tasa de falsos positivos\n",
    "    my $fpr = ($fp + $tn) > 0 ? $fp / ($fp + $tn) : 0;\n",
    "\n",
    "    return ($fpr, $tpr);\n",
    "};\n",
    "\n",
    "sml->add_to_class('sml', 'perf_metrics', $perf_metrics);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "973b45ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "*sml::trapz"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to calculate the integral using the trapezoid rule\n",
    "my $trapz = sub {\n",
    "    my ($self, $x_ref, $y_ref) = @_;\n",
    "\n",
    "    # Convertir a tensores\n",
    "    my $x = mx->nd->array($x_ref);\n",
    "    my $y = mx->nd->array($y_ref);\n",
    "\n",
    "    # Calcular diferencias y promedios\n",
    "    my $dx = mx->nd->slice($x, [1], [scalar @$x_ref]) - mx->nd->slice($x, [0], [scalar @$x_ref - 1]);\n",
    "    my $avg_y = (mx->nd->slice($y, [1], [scalar @$y_ref]) + mx->nd->slice($y, [0], [scalar @$y_ref - 1])) * 0.5;\n",
    "\n",
    "    # Área total como suma de áreas de trapecios\n",
    "    my $area = ($dx * $avg_y)->sum->asscalar;\n",
    "\n",
    "    return $area;\n",
    "};\n",
    "\n",
    "sml->add_to_class('sml', 'trapz', $trapz);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2378e010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: (0, 1, 0, 1, 0)\n",
      "predicted prob: (0.592837, 0.624829, 0.073848, 0.544891, 0.015118)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my ($dataset, $header) = sml->load_csv('./data/model.csv');\n",
    "\n",
    "my ($class, $predicted_prob) = (zip map {[ $_->[1], $_->[2] ]} @$dataset);\n",
    "\n",
    "printf \"class: %s\\n\", dump @$class[0 .. 4];\n",
    "printf \"predicted prob: %s\\n\", dump @$predicted_prob[0 .. 4];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a0a615f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad: 1.00, Especificidad: 0.99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate TPR and FPR for a specific threshold\n",
    "my ($fpr, $tpr) = sml->perf_metrics($class, $predicted_prob, 0.5);\n",
    "\n",
    "# Print sensitivity and specificity\n",
    "printf \"Sensibilidad: %.2f, Especificidad: %.2f\\n\", $tpr, 1 - $fpr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1fc7696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "00.050.10.150.20.250.30.350.40.450.50.550.60.650.70.750.80.850.90.951"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate TPR and FPR for various decision thresholds\n",
    "my @thresholds = map { $_ / 20 } (0 .. 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2613ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      "  [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    0.891891891891892,\n",
      "    0.77027027027027,\n",
      "    0.608108108108108,\n",
      "    0.378378378378378,\n",
      "    0.121621621621622,\n",
      "    0.0135135135135135,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "  ],\n",
      "  [\n",
      "    1,\n",
      "    0.855263157894737,\n",
      "    0.842105263157895,\n",
      "    0.657894736842105,\n",
      "    0.657894736842105,\n",
      "    0.25,\n",
      "    0.25,\n",
      "    0.0131578947368421,\n",
      "    0.0131578947368421,\n",
      "    0.0131578947368421,\n",
      "    0.0131578947368421,\n",
      "    0.0131578947368421,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "  ],\n",
      ")"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my ($fprs, $tprs) = (zip map {[sml->perf_metrics($class, $predicted_prob, $_)]} @thresholds);\n",
    "\n",
    "print dump $tprs, $fprs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3cd75b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.770270270270270.6081081081081080.3783783783783780.1216216216216220.0135135135135135000011110.8918918918918921111111"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the area under the ROC curve (AUC)\n",
    "# First, sort the points by ascending FPR\n",
    "my @sorted_indices = sort { $fprs->[$a] <=> $fprs->[$b] } 0 .. $#$fprs;\n",
    "my @sorted_fprs = @$fprs[@sorted_indices];\n",
    "my @sorted_tprs = @$tprs[@sorted_indices];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab8cbd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve (AUC): 0.981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then, calculate the AUC using the trapezoid rule\n",
    "my $auc = sml->trapz(\\@sorted_fprs, \\@sorted_tprs);\n",
    "printf \"Area under the ROC curve (AUC): %0.3f\\n\", $auc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "420d7e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "//# sourceURL=iperl-devel-plugin-chart-plotly.js\n",
       "            $('#Plotly').each(function(i, e) { $(e).attr('id', 'plotly') });\n",
       "\n",
       "            if (!window.Plotly) {\n",
       "                requirejs.config({\n",
       "                  paths: {\n",
       "                    plotly: ['https://cdn.plot.ly/plotly-latest.min']},\n",
       "                });\n",
       "                window.Plotly = {\n",
       "react : function (div, data, layout, config){\n",
       "                    require(['plotly'], function(plotly) {\n",
       "                      window.Plotly=plotly;\n",
       "Plotly.react(div, data, layout, config);                    });\n",
       "                  }\n",
       "                }\n",
       "            }\n",
       "</script>\n",
       "<div id=\"d7f176e3-3be9-11f0-b558-c8410b31db5a\"></div>\n",
       "\n",
       "<script>\n",
       "Plotly.react(document.getElementById('d7f176e3-3be9-11f0-b558-c8410b31db5a'),[{\"type\":\"scatter\",\"x\":[1,0.855263157894737,0.842105263157895,0.657894736842105,0.657894736842105,0.25,0.25,0.0131578947368421,0.0131578947368421,0.0131578947368421,0.0131578947368421,0.0131578947368421,0,0,0,0,0,0,0,0,0],\"y\":[1,1,1,1,1,1,1,1,1,1,1,0.891891891891892,0.77027027027027,0.608108108108108,0.378378378378378,0.121621621621622,0.0135135135135135,0,0,0,0],\"mode\":\"lines\",\"name\":\"ROC Curve\"},{\"name\":\"ROC Curve\",\"mode\":\"lines\",\"y\":[0,1],\"x\":[0,1],\"type\":\"scatter\"}] ,{\"yaxis\":{\"title\":\"True Positive Rate (TPR)\"},\"title\":\"ROC curve\",\"xaxis\":{\"title\":\"False Positive Rate (FPR)\"}} );\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the ROC curve using Chart::Plotly\n",
    "my $trace1 = new Chart::Plotly::Trace::Scatter(\n",
    "    x    => $fprs,\n",
    "    y    => $tprs,\n",
    "    mode => 'lines',\n",
    "    name => 'ROC Curve'\n",
    ");\n",
    "\n",
    "my $trace2 = new Chart::Plotly::Trace::Scatter(\n",
    "    x    => [0, 1],\n",
    "    y    => [0, 1],\n",
    "    mode => 'lines',\n",
    "    name => 'ROC Curve'\n",
    ");\n",
    "\n",
    "my $chart = new Chart::Plotly::Plot(\n",
    "    traces => [$trace1, $trace2],\n",
    "    layout => {\n",
    "        title => 'ROC curve',\n",
    "        xaxis => { title => 'False Positive Rate (FPR)' },\n",
    "        yaxis => { title => 'True Positive Rate (TPR)' }\n",
    "    }\n",
    ");\n",
    "\n",
    "# Show the graph directly in IPerl\n",
    "IPerl->display($chart);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f544a36",
   "metadata": {},
   "source": [
    "## 4.3 Extensions\n",
    "\n",
    "You have only seen a small sample of the most widely used performance metrics. There are many\n",
    "other performance metrics that you may require. Below is a list of 5 additional performance\n",
    "metrics that you may wish to implement to extend this tutorial\n",
    "* Precision for classification.\n",
    "* Recall for classification.\n",
    "* F1 for classification.\n",
    "* Area Under ROC Curve or AUC for classification.\n",
    "* Goodness of Fit or R 2 (R squared) for regression.\n",
    "\n",
    "## 4.4 Review\n",
    "\n",
    "In this tutorial, you discovered how to implement algorithm prediction performance metrics\n",
    "from scratch in Perl. Specifically, you learned:\n",
    "* How to implement and interpret classification accuracy.\n",
    "* How to implement and interpret the confusion matrix for classification problems.\n",
    "* How to implement and interpret mean absolute error for regression.\n",
    "* How to implement and interpret root mean squared error for regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPerl 0.011",
   "language": "perl",
   "name": "iperl"
  },
  "language_info": {
   "file_extension": ".pl",
   "mimetype": "text/x-perl",
   "name": "perl",
   "version": "5.32.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
